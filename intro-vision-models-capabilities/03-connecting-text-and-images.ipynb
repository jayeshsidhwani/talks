{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db215801-6a5d-48c9-bfb8-e488e2028f47",
   "metadata": {},
   "source": [
    "## Zero-shot / zero-data \n",
    "\n",
    "In 2021, OpenAI released CLIP model, which was trained on text & image pairs. \n",
    "\n",
    "This allowed the model to understand the relationships between visual concepts and their natural language descriptions.\n",
    "\n",
    "This was a major breakthrough in building applications that do not require pre-training, dramatically lowering the cost of building vision applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947cfe43-1be3-4b18-8e1e-ed7790412b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import AutoProcessor, CLIPModel\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e0b31a-534f-443d-ba0d-99becd7bacd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [ \"./assets/clip-text-image-encoding.jpg\", \"./assets/clip-text-image-prediction.jpg\" ]\n",
    "for image_path in image_paths:\n",
    "    image = Image.open(image_path)\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26b3b6c-b82e-4094-9f9d-ffebdf94bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f7201a-d736-4672-8559-fb14df8e73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image, prompts):\n",
    "    inputs = processor(\n",
    "        text = prompts,\n",
    "        images = image, \n",
    "        return_tensors = \"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image # sim_score\n",
    "    probs = logits_per_image.softmax(dim=1) # probabilities\n",
    "    \n",
    "    for ndx, prob in enumerate(probs[0]):\n",
    "        print(f\"Probability of {prompts[ndx]} is {(prob.item() * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c35833-976c-42b9-8c71-f7d3bc2b15e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [\n",
    "    \"./assets/pickleball.jpg\",\n",
    "    \"./assets/woman-apple.jpg\",\n",
    "    \"./assets/football-on-beach.jpg\"\n",
    "]\n",
    "\n",
    "for image_path in image_paths:\n",
    "    image = Image.open(image_path)\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30190dc9-e00e-4713-98a5-1e3aa7ba825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./assets/pickleball.jpg\")\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "prompts = [\n",
    "    \"people playing cricket\",\n",
    "    \"people playing pickleball\",\n",
    "    \"people playing football\"\n",
    "]\n",
    "\n",
    "evaluate(image, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0c2b2c-3fc5-40d7-bcd5-cc55b3d567e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./assets/woman-apple.jpg\")\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "prompts = [\n",
    "    \"woman eating apple\",\n",
    "    \"man eating apple\",\n",
    "    \"woman eating banana\",\n",
    "    \"man cooking fish\",\n",
    "]\n",
    "\n",
    "evaluate(image, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a4226e-f81e-4268-9e31-4998ce77e0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./assets/football-on-beach.jpg\")\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "prompts = [\n",
    "    \"people playing football on a beach\",\n",
    "    \"people playing football in a ground\",\n",
    "    \"people playing hockey on a beach\",\n",
    "    \"people running on a beach\",\n",
    "]\n",
    "\n",
    "evaluate(image, prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c424c2c-963b-48e1-a574-3898458cfedf",
   "metadata": {},
   "source": [
    "# Text Association -> Generation\n",
    "\n",
    "CLIP is fundamentally an encoder that creates text and image embeddings in a shared embedding space, enabling powerful associations between visual and textual content.\n",
    "\n",
    "However, the utility increases dramatically when encoders like CLIP are paired with generative language models. \n",
    "\n",
    "This combination moves beyond simple image-text association to actually generating contextual descriptions based on visual content and additional user context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed2e7ca-bf2c-4fee-ae6c-45b7260a4ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cbce6b-6151-4199-9365-1f9ac2cf551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_api_key = os.environ['GEMINI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7357478e-ef56-488e-a24d-b5080e7f0078",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=gemini_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e9ba09-3b9a-4425-ad36-b6ebc5ddc2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file = client.files.upload(file=\"./assets/football-on-beach.jpg\")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=[image_file, \"Explain what is happening in the image\"]\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2cc048-c1c6-4530-b86b-b6823a4563c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
