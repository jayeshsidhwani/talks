{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "010e7143-93e1-4125-9959-bef703c34bdf",
   "metadata": {},
   "source": [
    "# Video Understanding\n",
    "\n",
    "Video understanding presents significantly greater challenges than static image analysis due to the addition of temporal dynamics. Models must not only recognize objects and scenes within individual frames but also understand how they change, move, and interact over time.\n",
    "\n",
    "## Key Challenges:\n",
    "\n",
    "- Computational complexity: Processing multiple frames together requires substantially more memory and processing power\n",
    "- Temporal context limitations: Most models can only analyze limited time windows, missing long-term patterns or dependencies\n",
    "\n",
    "In this notebook, we will look at a video vision transformer-based model, called ViVit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5964925-27cd-4a92-8eb5-78e0a458c3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f55d1d0-3f2f-4abd-802c-a922681e92a1",
   "metadata": {},
   "source": [
    "## Spatial and Temporal Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4d2701-2a5a-4a2e-90e8-1c6cbe592dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tubelet_spatial_path = \"./assets/tubelet-spatial.png\"\n",
    "tubelet_temporal_path = \"./assets/tubelet-temporal.png\"\n",
    "\n",
    "plt.title(\"ViVit - Spatial and Temporal information\")\n",
    "image = cv2.imread(tubelet_spatial_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "image = cv2.imread(tubelet_temporal_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c580a32-ae35-468a-b8b0-83f5bfd6d4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import VivitImageProcessor, VivitForVideoClassification, AutoImageProcessor, VideoMAEForVideoClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4564672-bcdc-4f83-b0ff-4e23e6d0a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video(container, indices):\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b8bf2-0a66-42f9-9fb6-e5bea2005eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    temporal_window = int(clip_len * frame_sample_rate)\n",
    "    \n",
    "    # Handle edge case\n",
    "    if seg_len <= temporal_window:\n",
    "        return np.linspace(0, seg_len - 1, num=clip_len).astype(np.int64)\n",
    "    \n",
    "    end_idx = np.random.randint(temporal_window, seg_len)\n",
    "    start_idx = end_idx - temporal_window\n",
    "    \n",
    "    indices = np.linspace(start_idx, end_idx - 1, num=clip_len).astype(np.int64)\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23f9fcf-d74e-4f60-9d57-9b5ff03cee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
    "# model = VivitForVideoClassification.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fe9aa1-71c7-41fd-b3a5-b18a2536a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/willprice/f19da185c9c5f32847134b87c1960769\n",
    "\n",
    "video_paths = [\n",
    "    \"./assets/basketball-top-5.mp4\",\n",
    "    \"./assets/dancing.mp4\",\n",
    "    \"./assets/football.mp4\",\n",
    "    \"./assets/pickleball.mp4\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf96e99a-d6c0-4f48-ae27-cbce898c0889",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = video_paths[1]\n",
    "print(f\"Processing #{video_path}\")\n",
    "\n",
    "container = av.open(video_path)\n",
    "\n",
    "indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n",
    "video = read_video(container=container, indices=indices)\n",
    "\n",
    "inputs = image_processor(list(video), return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "predicted_label = logits.argmax(-1).item()\n",
    "\n",
    "print(f\"{video_path} has label: {model.config.id2label[predicted_label]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca4e0ce-79d4-46d6-8187-52f5fd6a0b5b",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n",
    "- These models have a very small context window (8, 16, 32, 96 frames)\n",
    "- Models are not generalisable\n",
    "- Pre-trained datasets are very limited, specific to certain industries, and are not actively maintained \n",
    "- Training is expensive and requires significant amount of task-specific annotated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bd67e2-17bc-447f-a30b-3950328a31e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
