{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87cd5a9-1eb6-4ceb-ab21-d97672eb09de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d96586a-fbc6-4ff4-a94b-d2941132a9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./assets/cat.jpeg\"\n",
    "image_path2 = \"./assets/cat-2.jpg\"\n",
    "video_path = \"./assets/cat.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec837fa-ae09-44f8-a379-0a3ecc6ab53b",
   "metadata": {},
   "source": [
    "## Multi-Modal Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad8ca69-537e-4900-8818-9ecf19769d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "image = cv2.imread(image_path)\n",
    "print(f\"Image Shape (H,W,channel): {image.shape}\")\n",
    "print(f\"Pixel 0,0 {image[0][0]}\")\n",
    "print()\n",
    "\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc940de-ac13-472a-969a-a0312328b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "video = cv2.VideoCapture(video_path)\n",
    "frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "print(f\"Total Frames: {frame_count}\")\n",
    "print(f\"Frames per second: {int(fps)}\")\n",
    "\n",
    "\n",
    "if not video.isOpened():\n",
    "    print(\"Failed to open the video\")\n",
    "\n",
    "frames = []\n",
    "for i in range(frame_count):\n",
    "    ok, frame = video.read()\n",
    "    if ok:\n",
    "        if i % 30 == 0: \n",
    "            frames.append(frame)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "for ndx, frame in enumerate(frames):\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    plt.title(f\"Second #{ndx}\")\n",
    "    plt.imshow(frame)\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db881f9-36b1-4d0e-b987-a6f40795b006",
   "metadata": {},
   "source": [
    "## How are they represented?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280b329f-8e2c-4fe2-b660-0281597ba4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, CLIPModel, CLIPProcessor\n",
    "import torch\n",
    "\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee940e0-687d-41b9-826e-1658e37ccac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image_inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "    image_embedding = clip_model.get_image_features(**image_inputs)\n",
    "    print(f\"Shape: {image_embedding.shape}\")\n",
    "    print(f\"Sample: {image_embedding[0][:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ba0a9a-ef0a-45e4-8985-47291d1996df",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "video.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "for i in range(0, frame_count, frame_count//9):\n",
    "    video.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "    ok, frame = video.read()\n",
    "    if ok:\n",
    "        frames.append(frame)\n",
    "\n",
    "frame_embeddings = []\n",
    "for frame in frames:\n",
    "    with torch.no_grad():\n",
    "        frame_input = clip_processor(images=frame, return_tensors=\"pt\")\n",
    "        frame_emb = clip_model.get_image_features(**frame_input)\n",
    "        frame_embeddings.append(frame_emb)\n",
    "\n",
    "print(f\"Shape: {frame_embeddings[0].shape}\")\n",
    "print(f\"Sample: {frame_embeddings[0][0][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a200d5-eadb-4cd0-a640-813a6cc3eb59",
   "metadata": {},
   "source": [
    "### How to choose a tokenizer?\n",
    "\n",
    "Different models produce different types of embeddings based on what they were designed to learn. For instance, CLIP's image encoder creates embeddings that capture general semantic meaning - understanding objects, scenes, and concepts that can be described in natural language. In contrast, specialized models like deep-person-reid are trained for specific tasks and generate embeddings that emphasize particular features. Deep-person-reid focuses on extracting person-specific characteristics useful for identifying and distinguishing between individuals, so its embeddings highlight attributes like clothing, pose, and appearance details rather than general scene understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c923c960-bb30-45db-a79a-1e25fce42466",
   "metadata": {},
   "source": [
    "# Similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf34af25-daff-4905-b418-ac06ef231062",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = Image.open(image_path)\n",
    "image2 = Image.open(image_path2)\n",
    "\n",
    "plt.title(\"Image 1\")\n",
    "plt.imshow(image1)\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Image 2\")\n",
    "plt.imshow(image2)\n",
    "plt.show()\n",
    "\n",
    "with torch.no_grad():\n",
    "    image1_inputs = clip_processor(images=image1, return_tensors=\"pt\")\n",
    "    image1_embedding = clip_model.get_image_features(**image1_inputs)\n",
    "    image2_inputs = clip_processor(images=image2, return_tensors=\"pt\")\n",
    "    image2_embedding = clip_model.get_image_features(**image2_inputs)\n",
    "\n",
    "image_sim_dist = torch.nn.functional.cosine_similarity(image1_embedding, image2_embedding)\n",
    "print(f\"Similarity: {image_sim_dist.item()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce389d33-c4d1-43c1-b096-70ad564ba1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae35b4ad-d9c5-4691-a692-5c5ba9f07fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
