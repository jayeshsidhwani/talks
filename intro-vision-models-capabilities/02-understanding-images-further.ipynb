{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5931ec93-33ea-4e90-a798-83610ae94394",
   "metadata": {},
   "source": [
    "## Image Semantics\n",
    "In the previous notebook on [understanding modalities](understanding-modalities.ipynb), we explored how computers process images and how different models can be tailored to extract specific types of information based on our requirements.\n",
    "Building on that foundation, computer vision offers several higher-level techniques that help us analyze what's actually happening within an image. Three fundamental approaches are object detection, segmentation, and tracking. These methods allow us to move beyond basic image understanding to identify specific elements and their precise locations within the visual data. This detailed spatial information becomes the building blocks for practical applications in autonomous vehicles, medical imaging, surveillance systems, and many other real-world domains.\n",
    "\n",
    "## Key Techniques\n",
    "- Object Detection: Locates and identifies objects within an image, typically by drawing bounding boxes around them\n",
    "- Object Segmentation: Creates precise pixel-level boundaries around objects, showing their exact shapes rather than just rectangular boxes\n",
    "- Object Tracking: Follows specific objects across multiple frames in a video sequence, maintaining consistent identification over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc79e780-b82a-43e8-bee9-93096de35f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "from ultralytics import YOLO\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595f160c-e43b-434d-9513-670d183e2cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./assets/crowded-city.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5c8bab-3502-4f85-af7f-8b30df08c6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(image_path)\n",
    "\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56512a38-5944-4590-ac57-7d65f0b7606b",
   "metadata": {},
   "source": [
    "## Object Detection & Use-Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ece4d7e-e126-460e-bba7-608558ccd577",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_annotator = sv.BoxAnnotator()\n",
    "label_annotator = sv.LabelAnnotator()\n",
    "mask_annotator = sv.MaskAnnotator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ecb61d-2d82-4a20-ba1e-d392d0175c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the models with pre-trained weights on the COCO dataset\n",
    "# https://gist.github.com/AruniRC/7b3dadd004da04c80198557db5da4bda\n",
    "\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "image = cv2.imread(image_path)\n",
    "results = model.predict(image, conf=0.5, verbose=False)[0]\n",
    "detections = sv.Detections.from_ultralytics(results)\n",
    "\n",
    "labels = [\n",
    "    f\"{class_name} {confidence:.2f}\"\n",
    "    for class_name, confidence\n",
    "    in zip(detections['class_name'], detections.confidence)\n",
    "]\n",
    "annotated_image = box_annotator.annotate(scene=image, detections=detections)\n",
    "annotated_image = label_annotator.annotate(scene=annotated_image, detections=detections, labels=labels)\n",
    "color_corrected_annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
    "plt.title(\"Annotated Image\")\n",
    "plt.imshow(color_corrected_annotated_image)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Sample detections: {detections.xyxy[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b77427-e49a-43bf-90fc-fb13c1f4e159",
   "metadata": {},
   "source": [
    "## Object Segmentation & Use-Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a97c64-a6dd-442d-8f05-1d0774268e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8n-seg.pt\")\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "results = model.predict(image, conf=0.3, verbose=False)[0]\n",
    "detections = sv.Detections.from_ultralytics(results)\n",
    "\n",
    "labels = [\n",
    "    f\"{class_name} {confidence:.2f}\"\n",
    "    for class_name, confidence\n",
    "    in zip(detections['class_name'], detections.confidence)\n",
    "]\n",
    "annotated_image = mask_annotator.annotate(scene=image, detections=detections)\n",
    "annotated_image = label_annotator.annotate(scene=annotated_image, detections=detections, labels=labels)\n",
    "color_corrected_annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.title(\"Annotated Image\")\n",
    "plt.imshow(color_corrected_annotated_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afbd84e-04be-48ab-9623-7f13b1bf1360",
   "metadata": {},
   "source": [
    "### Object Tracking & Use-Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7336a1-ef90-44b5-8ea0-e7c0a16868ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "def callback(frame: np.ndarray, _:int) -> np.ndarray:\n",
    "    results = model(frame, verbose=False)[0]\n",
    "    detections = sv.Detections.from_ultralytics(results)\n",
    "    return box_annotator.annotate(frame.copy(), detections=detections)\n",
    "\n",
    "Video(\"./assets/people-crossing-signal.mp4\")\n",
    "\n",
    "sv.process_video(\n",
    "    source_path=\"./assets/people-crossing-signal.mp4\",\n",
    "    target_path=\"./assets/people-crossing-signal-output.mp4\",\n",
    "    callback=callback\n",
    ")\n",
    "\n",
    "Video(\"./assets/people-crossing-signal-output.mp4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ae643f-daf8-4775-bc32-fec2bb38bbeb",
   "metadata": {},
   "source": [
    "## Limitations of the approach\n",
    "\n",
    "Most pre-trained models excel within their intended domains but struggle when applied to novel scenarios.\n",
    "\n",
    "Adapting these models to new domains or specific use cases requires substantial labeled datasets - often thousands or tens of thousands of annotated examples. \n",
    "\n",
    "Ideally, the systems should adapt to new contexts as easily as human does. We can recognize objects we've never seen before by understanding their basic properties and relationships. \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
